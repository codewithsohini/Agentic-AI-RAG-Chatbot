# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RRo30W_pl7oPTeH8fsVQRRX1J8qnPOP1
"""



import streamlit as st
import os
import json
from typing import TypedDict, List
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END


st.set_page_config(page_title="Agentic AI Assistant", page_icon="ðŸ¤–")
st.title("ðŸ¤– Agentic AI eBook Assistant")

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(
    persist_directory="./chroma_db", 
    embedding_function=embeddings
)

class State(TypedDict):
    question: str
    context: List[str]
    answer: str
    confidence_score: float

def retrieve(state: State):
    query = state["question"]
    docs = vectorstore.similarity_search(query, k=3)
    content = [doc.page_content for doc in docs]
    return {"context": content}

def generate(state: State):
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("API Key not found in Secrets!")
        return {"answer": "Error: API Key missing.", "confidence_score": 0.0}
    
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0, google_api_key=api_key)
    
    prompt = ChatPromptTemplate.from_template("""
    You are an AI Assistant for the Agentic AI eBook. Answer ONLY using the context.
    Context: {context}
    Question: {question}
    Return JSON: {{"answer": "...", "confidence_score": 0.0}}
    """)
    
    chain = prompt | llm
    response = chain.invoke({"context": "\n\n".join(state["context"]), "question": state["question"]})
    
    content = response.content.replace("```json", "").replace("```", "").strip()
    try:
        data = json.loads(content)
        return {"answer": data.get("answer"), "confidence_score": data.get("confidence_score")}
    except:
        return {"answer": response.content, "confidence_score": 0.5}

workflow = StateGraph(State)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)
rag_app = workflow.compile()

user_input = st.text_input("Ask a question about the Agentic AI eBook:")

if st.button("Ask Agent"):
    if user_input:
        with st.spinner("Analyzing eBook..."):
            result = rag_app.invoke({"question": user_input})
            
            # Display Results
            st.markdown(f"### Answer")
            st.write(result['answer'])
            
            st.metric("Confidence Score", f"{result['confidence_score'] * 100}%")
            
            with st.expander("View Source Context"):
                for i, chunk in enumerate(result['context']):
                    st.info(f"Source {i+1}:\n{chunk}")
    else:
        st.warning("Please enter a question.")
